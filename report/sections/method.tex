\mypar{Locality optimizations}
A first analysis of our naive implementation revealed that the creation and destruction of \texttt{BigInt} objects caused millions of \texttt{malloc}/\texttt{free} operations to be executed for each key pair. This results in poor locality and lot of time spent in OS calls.

Our solution consist in a single contiguous buffer with enough space for all the \texttt{BigInt} objects required The buffer is allocated at initialization time and each function can access the single objects using tags like in the example below:

\begin{lstlisting}[frame=single, mathescape=true, captionpos=b, caption=Access by tag example]
BigInt shared_v = GET_BIGINT_PTR(
    BI_ECDH_SHAREDV_TAG);
\end{lstlisting}

For the scope of our project we assume that the maximum size of $p$ is $521$ bits. Including the header information we conclude that each \texttt{BigInt} object fits in at most $192$ bytes. Since there are $134$ distinct objects we get a total of $25.8KB$, that fits in the $32KB$ L1 cache of our target system.

With this approach we have great spatial and temporal locality, minimizing the number of cache misses to the compulsory ones.

\mypar{Montgomery multiplication}
Running our application with callgrind after introducing Jacobian coordinates we noticed that the bottleneck of our algorithm is the multiplication in Montgomery space, consuming more than $80\%$ of CPU time.



\mypar{ADX vs AVX2}  

Now comes the ``beef'' of the paper, where you explain what you
did. Again, organize it in paragraphs with titles. As in every section
you start with a very brief overview of the section.

For this class, explain all the optimizations you performed. This mean, you first very briefly
explain the baseline implementation, then go through locality and other optimizations, and finally SSE (every project will be slightly different of course). Show or mention relevant analysis or assumptions. A few examples: 1) Profiling may lead you to optimize one part first; 2) bandwidth plus data transfer analysis may show that it is memory bound; 3) it may be too hard to implement the algorithm in full generality: make assumptions and state them (e.g., we assume $n$ is divisible by 4; or, we consider only one type of input image); 4) explain how certain data accesses have poor locality. Generally, any type of analysis adds value to your work.

As important as the final results is to show that you took a structured, organized approach to the optimization and that you explain why you did what you did.

Mention and cite any external resources including library or other code.

Good visuals or even brief code snippets to illustrate what you did are good. Pasting large amounts of code to fill the space is not good.